# ğŸš€ Obezite Tahmin Sistemi + Llama AI Chatbot Kurulum Rehberi

Bu rehber, obezite tahmin sisteminize Llama AI tabanlÄ± saÄŸlÄ±k chatbot'u entegre etmek iÃ§in adÄ±m adÄ±m kurulum talimatlarÄ±nÄ± iÃ§erir.

## ğŸ“‹ Gereksinimler

### Sistem Gereksinimleri
- **Ä°ÅŸletim Sistemi:** Windows 10/11, macOS, Linux
- **Python:** 3.8 veya Ã¼zeri
- **RAM:** En az 8GB (Llama iÃ§in 16GB Ã¶nerilen)
- **Disk AlanÄ±:** En az 10GB boÅŸ alan

### YazÄ±lÄ±m Gereksinimleri
- Python 3.8+
- pip (Python paket yÃ¶neticisi)
- Git (isteÄŸe baÄŸlÄ±)

## ğŸ”§ Kurulum AdÄ±mlarÄ±

### 1. Python BaÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± Kurun

```bash
# Proje dizinine gidin
cd "c:\Users\Emre\Desktop\Obesity"

# Sanal ortam oluÅŸturun (Ã¶nerilen)
python -m venv obesity_env

# Sanal ortamÄ± aktif edin
# Windows:
obesity_env\Scripts\activate
# macOS/Linux:
source obesity_env/bin/activate

# BaÄŸÄ±mlÄ±lÄ±klarÄ± kurun
pip install -r requirements.txt
```

### 2. Ollama Kurulumu ve YapÄ±landÄ±rmasÄ±

#### Windows Ä°Ã§in:

```bash
# 1. Ollama'yÄ± indirin ve kurun
# https://ollama.ai/download adresinden Windows installer'Ä± indirin

# 2. Komut satÄ±rÄ±ndan Ollama'yÄ± test edin
ollama --version

# 3. Llama3.1 modelini kurun (yaklaÅŸÄ±k 4.7GB)
ollama pull llama3.1

# 4. Model kurulumunu doÄŸrulayÄ±n
ollama list
```

#### macOS Ä°Ã§in:

```bash
# 1. Homebrew ile kurun
brew install ollama

# veya
# https://ollama.ai/download adresinden macOS installer'Ä± indirin

# 2. Llama3.1 modelini kurun
ollama pull llama3.1

# 3. Servisi baÅŸlatÄ±n
brew services start ollama
```

#### Linux Ä°Ã§in:

```bash
# 1. Ollama'yÄ± kurun
curl -fsSL https://ollama.ai/install.sh | sh

# 2. Llama3.1 modelini kurun
ollama pull llama3.1

# 3. Servis olarak baÅŸlatÄ±n
sudo systemctl enable ollama
sudo systemctl start ollama
```

### 3. Ollama Servisini BaÅŸlatÄ±n

```bash
# Ollama servisini arka planda baÅŸlatÄ±n
ollama serve

# Alternatif olarak (Windows'ta):
# BaÅŸlat menÃ¼sÃ¼nden "Ollama" uygulamasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±n
```

### 4. Flask Backend'i BaÅŸlatÄ±n

```bash
# Backend dizinine gidin
cd backend

# Flask uygulamasÄ±nÄ± baÅŸlatÄ±n
python app.py

# veya
flask run --host=0.0.0.0 --port=5000
```

### 5. Frontend'i Ã‡alÄ±ÅŸtÄ±rÄ±n

```bash
# Proje ana dizininde
# Basit HTTP sunucusu baÅŸlatÄ±n (Python ile)
cd frontend
python -m http.server 8080

# veya Node.js varsa
npx http-server -p 8080

# TarayÄ±cÄ±da aÃ§Ä±n: http://localhost:8080
```

## ğŸ§ª Test ve DoÄŸrulama

### 1. Backend API Testi

```bash
# API saÄŸlÄ±k kontrolÃ¼
curl http://localhost:5000/

# Chat servisi durumu
curl http://localhost:5000/api/chat/status
```

### 2. Ollama Testi

```bash
# DoÄŸrudan Ollama testi
curl http://localhost:11434/api/generate \
  -d '{"model": "llama3.1", "prompt": "Merhaba, nasÄ±lsÄ±n?", "stream": false}'
```

### 3. Fonksiyonel Test

1. **Obezite Tahmini:**
   - `http://localhost:8080` adresine gidin
   - Form doldurun ve tahmin yapÄ±n
   - SonuÃ§larÄ±n gÃ¶rÃ¼ntÃ¼lendiÄŸini doÄŸrulayÄ±n

2. **Chat Entegrasyonu:**
   - Tahmin sonrasÄ± "SaÄŸlÄ±k AsistanÄ± ile KonuÅŸ" butonuna tÄ±klayÄ±n
   - Chat sayfasÄ±nÄ±n aÃ§Ä±ldÄ±ÄŸÄ±nÄ± kontrol edin
   - Mesaj gÃ¶nderip yanÄ±t aldÄ±ÄŸÄ±nÄ±zÄ± doÄŸrulayÄ±n

## ğŸš¨ YaygÄ±n Sorunlar ve Ã‡Ã¶zÃ¼mler

### Sorun 1: Ollama BaÄŸlantÄ± HatasÄ±

**Belirtiler:**
- "Ollama baÄŸlantÄ± hatasÄ±" mesajÄ±
- Chat'te sadece fallback mesajlar

**Ã‡Ã¶zÃ¼m:**
```bash
# Ollama servisini yeniden baÅŸlatÄ±n
ollama serve

# Port kontrolÃ¼ yapÄ±n
netstat -an | grep 11434

# Windows'ta:
netstat -an | findstr 11434
```

### Sorun 2: Model BulunamadÄ± HatasÄ±

**Belirtiler:**
- "Model mevcut deÄŸil" uyarÄ±sÄ±
- Llama yanÄ±tlarÄ± alÄ±namÄ±yor

**Ã‡Ã¶zÃ¼m:**
```bash
# Mevcut modelleri listeleyin
ollama list

# Llama3.1 yoksa tekrar kurun
ollama pull llama3.1

# Model boyutu kontrolÃ¼ (yaklaÅŸÄ±k 4.7GB olmalÄ±)
```

### Sorun 3: CORS HatasÄ±

**Belirtiler:**
- Browser console'da CORS hatasÄ±
- Frontend-Backend iletiÅŸim sorunu

**Ã‡Ã¶zÃ¼m:**
```python
# backend/app.py dosyasÄ±nda CORS ayarlarÄ±nÄ± kontrol edin
from flask_cors import CORS
CORS(app, origins=["http://localhost:8080", "http://127.0.0.1:8080"])
```

### Sorun 4: Port Ã‡akÄ±ÅŸmasÄ±

**Belirtiler:**
- "Port kullanÄ±mda" hatasÄ±
- Servislerin baÅŸlamamasÄ±

**Ã‡Ã¶zÃ¼m:**
```bash
# KullanÄ±lan portlarÄ± kontrol edin
# Windows:
netstat -ano | findstr :5000
netstat -ano | findstr :8080

# Linux/macOS:
lsof -i :5000
lsof -i :8080

# Alternatif portlar kullanÄ±n
python app.py --port=5001
python -m http.server 8081
```

## ğŸ“Š Performans Optimizasyonu

### Llama Model Optimizasyonu

```bash
# Daha kÃ¼Ã§Ã¼k model kullanÄ±n (daha hÄ±zlÄ±)
ollama pull llama3.1:8b

# Model parametrelerini ayarlayÄ±n
# llama_integration.py dosyasÄ±nda:
"options": {
    "temperature": 0.7,    # YaratÄ±cÄ±lÄ±k (0.1-1.0)
    "top_p": 0.9,         # Ã‡eÅŸitlilik (0.1-1.0)
    "max_tokens": 1000    # Maksimum kelime sayÄ±sÄ±
}
```

### Cache AyarlarÄ±

```python
# backend/app.py'ye cache ekleyin
from flask_caching import Cache

cache = Cache(app, config={'CACHE_TYPE': 'simple'})

@cache.memoize(timeout=300)  # 5 dakika cache
def get_health_recommendations(prediction_data, user_input):
    # Mevcut kod...
```

## ğŸ”’ GÃ¼venlik Ã–nerileri

### 1. API GÃ¼venliÄŸi

```python
# Rate limiting ekleyin
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address

limiter = Limiter(
    app,
    key_func=get_remote_address,
    default_limits=["100 per hour"]
)

@app.route('/api/chat', methods=['POST'])
@limiter.limit("10 per minute")
def chat_with_assistant():
    # Mevcut kod...
```

### 2. Input Validation

```python
# KullanÄ±cÄ± giriÅŸlerini doÄŸrulayÄ±n
def validate_input(data):
    if not isinstance(data.get('message'), str):
        raise ValueError("GeÃ§ersiz mesaj formatÄ±")
    
    if len(data['message']) > 1000:
        raise ValueError("Mesaj Ã§ok uzun")
    
    return True
```

## ğŸ“± Mobil Uyumluluk

CSS responsive tasarÄ±m zaten dahil, ancak ek optimizasyonlar:

```css
/* chat.css'e eklenebilir */
@media (max-width: 480px) {
    .chat-sidebar {
        position: fixed;
        left: -300px;
        transition: left 0.3s;
        z-index: 1000;
    }
    
    .chat-sidebar.open {
        left: 0;
    }
}
```

## ğŸ”„ Otomatik BaÅŸlatma

### Windows Servis Olarak Ã‡alÄ±ÅŸtÄ±rma

```batch
@echo off
cd /d "c:\Users\Emre\Desktop\Obesity"
call obesity_env\Scripts\activate
cd backend
python app.py
```

### Linux Systemd Servisi

```ini
[Unit]
Description=Obesity Prediction API
After=network.target

[Service]
Type=simple
User=your-username
WorkingDirectory=/path/to/obesity/backend
Environment=PATH=/path/to/obesity/obesity_env/bin
ExecStart=/path/to/obesity/obesity_env/bin/python app.py
Restart=always

[Install]
WantedBy=multi-user.target
```

## ğŸ“ˆ Monitoring ve Logging

```python
# backend/app.py'ye logging ekleyin
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('obesity_api.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)
```

## ğŸ¯ SonuÃ§

Bu kurulum rehberini takip ederek:

âœ… **BaÅŸarÄ±yla kurulmuÅŸ olacak:**
- Flask backend API'si
- Llama AI entegrasyonu
- Modern chat arayÃ¼zÃ¼
- Obezite tahmin sistemi entegrasyonu

âœ… **Ã–zellikler:**
- GerÃ§ek zamanlÄ± AI sohbeti
- KiÅŸiselleÅŸtirilmiÅŸ saÄŸlÄ±k Ã¶nerileri
- Streaming chat deneyimi
- Responsive mobil tasarÄ±m
- Fallback sistemi (Llama Ã§alÄ±ÅŸmasa bile temel Ã¶neriler)

## ğŸ†˜ Destek

Sorun yaÅŸadÄ±ÄŸÄ±nÄ±zda:

1. **Log dosyalarÄ±nÄ± kontrol edin**
2. **Port Ã§akÄ±ÅŸmasÄ± olup olmadÄ±ÄŸÄ±na bakÄ±n**
3. **Ollama servisinin Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun**
4. **Browser console'da hata mesajlarÄ±nÄ± inceleyin**

**Ä°letiÅŸim:** GitHub Issues Ã¼zerinden sorularÄ±nÄ±zÄ± sorabilirsiniz.

---

**âš ï¸ Ã–nemli UyarÄ±:** Bu sistem eÄŸitim ve demo amaÃ§lÄ±dÄ±r. GerÃ§ek tÄ±bbi tanÄ± koymaz, sadece genel saÄŸlÄ±k Ã¶nerileri sunar. Ciddi saÄŸlÄ±k sorunlarÄ± iÃ§in mutlaka doktorunuza baÅŸvurun.